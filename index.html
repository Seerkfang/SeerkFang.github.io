<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yunhao Fang 方云浩</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yunhao Fang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="files/CV_YunhaoFang.pdf">CV&nbsp;(Dec&nbsp;2024)</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yunhao Fang 方云浩</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photos/fyh.jpg" alt="Yunaho Fang" width="160px" height="160px" />&nbsp;</td>
<td align="left"><p>Research Scientist Intern,<br />
Nvidia<br />
Email: seerkfang [@] gmail [DOT] com<br />
<a href="https://twitter.com/home">Twitter</a> /
<a href="https://github.com/Seerkfang">Github</a> / 
<a href="https://www.linkedin.com/in/yunhao-fang-8b318221a/">LinkedIn</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I'm a Research Scientist Intern at Nvidia, advised by Dr. <a href="https://research.nvidia.com/person/yao-lu-jason">Jason Lu</a> and Prof. <a href="https://hanlab.mit.edu/songhan">Song Han</a>, and a core contributor to Nvidia’s multimodal model, <a href="https://github.com/NVlabs/VILA">VILA</a>.</p>
<p>I hold a Master's degree from the <a href="https://cse.ucsd.edu/">Department of Computer Science and Engineering</a> at the <a href="https://www.ucsd.edu/">University of California San Diego</a>, where I was fortunate to be advised by Prof. <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a>. Before that, I earned my B.Eng. in Electronic Engineering from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. I have also spent time at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Shanghai AI Laboratory</a>, as the maintainer of the opensource codebase <a href="https://github.com/open-mmlab/mmtracking">mmtracking</a>.</p>
<p>My long-term research goal is to develop automated learning system that integrate closed-loop data pipelines, efficient algorithms, and robust evaluation tools, advancing the frontiers of <b>multimodal intelligence</b>.</p>
<h2>Research interests</h2>
<p>My research interests include </p>
<ul>
<li><p><b>Perception</b></p>
<ul>
<li><p>Generalized Representation</p>
</li>
<li><p>Synergy between Understanding and Generation</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Reasoning and Common Sense</b></p>
<ul>
<li><p>Concept Emergence and Common Sense</p>
</li>
<li><p>Advanced Reasoning for Scientific Discoveries</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Generative Modeling</b></p>
<ul>
<li><p>Efficient World Model</p>
</li>
<li><p>Learning from (Human or AI) Feedback</p>
</li>
</ul>

</li>
</ul>
<h2>Selected Publications &amp; Preprints</h2>
<p>Papers sorted by years. The full list is available on <a href="https://scholar.google.com/citations?user=vrYHLMgAAAAJ&amp;hl=en">Google Scholar</a>.</p>
<h3>2024</h3>
<table class="imgtable"><tr><td>
<img src="figures/fyh/vila^2_teaser.jpg" alt="vila^2" width="240px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/pdf/2407.17453.pdf">VILA^2: VLM Augmented VLM with Self-Improvement</a></b><br />
<b>Yunhao Fang*</b>, Ligeng Zhu*, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, Hongxu Yin<br />
<i>In Submission</i><br /></p>
</td></tr></table>
<h3>2023</h3>
<table class="imgtable"><tr><td>
<img src="figures/fyh/verify_cot.jpg" alt="verify_cot" width="240px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/pdf/2306.03872.pdf">Deductive Verification of Chain-of-Thought Reasoning</a></b><br />
Zhan Ling*, <b>Yunhao Fang*</b>, Xuanlin Li, Zhiao Huang, Hao Su<br />
<i>Neural Information Processing Systems <b>(NeurIPS)</b> 2023</i><br />
<a href="https://github.com/lz1oceani/verify_cot">[Code]</a><br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="figures/fyh/vlm_distillation.jpg" alt="vlm_distillation" width="240px" />&nbsp;</td>
<td align="left"><p><b><a href="https://arxiv.org/pdf/2307.03135">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability</a></b><br />
Xuanlin Li*, <b>Yunhao Fang*</b>, Minghua Liu, Zhan Ling, Zhuowen Tu, Hao Su<br />
<i>International Conference on Computer Vision <b>(ICCV)</b> 2023</i><br />
<a href="https://github.com/xuanlinli17/large_vlm_distillation_ood">[Code]</a><br /></p>
</td></tr></table>
<h2>Professional Services</h2>
<ul>
<li><p><b>Conference Reviewer</b>: ECCV 2024, ICLR 2024, CVPR 2024</p>
</li>
</ul>
<h2>Teaching</h2>
<ul>
<li><p>Teaching Assistant: CSE 275: Deep Learning for 3D Data at UC San Diego, Fall 2023</p>
</li>
</ul>
<h2>Awards </h2>
<ul>
<li><p>China National Scholarship, 2022
<br /></p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
